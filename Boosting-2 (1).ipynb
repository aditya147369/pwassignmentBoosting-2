{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5277b8d-36bb-48ea-a1c8-5cfbe2568d7c",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ec084-8c8a-4a30-bdc8-04a2558006df",
   "metadata": {},
   "source": [
    "Ans - Gradient Boosting Regression is a powerful machine learning technique used for predicting numerical target variables. It builds an ensemble of weak prediction models, typically decision trees, in a sequential manner. Each new tree is trained to correct the errors of the previous trees, leading to a strong overall prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8e1a3-b066-4934-a0cc-1c3c12be1d5b",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6db312-1515-42b2-92c1-9940cab61c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.00000000564406\n",
      "R-squared: -3.5000000007055077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        predictions = np.full(len(X), np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "          \n",
    "            residuals = y - predictions\n",
    "     \n",
    "            model = DecisionTreeRegressor()\n",
    "            model.fit(X, residuals)\n",
    "            \n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            self.models.append(model)\n",
    "            self.weights.append(self.learning_rate)\n",
    "    \n",
    "    def predict(self, X):\n",
    "    \n",
    "        predictions = np.zeros(len(X))\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            predictions += weight * model.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    def fit(self, X, y):\n",
    "        self.feature_index, self.threshold, self.value = self._find_best_split(X, y)\n",
    "        if self.feature_index is None:\n",
    "            self.value = np.mean(y)\n",
    "            return\n",
    "        \n",
    "        left_indices = X[:, self.feature_index] < self.threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        self.left = DecisionTreeRegressor()\n",
    "        self.left.fit(X[left_indices], y[left_indices])\n",
    "        \n",
    "        self.right = DecisionTreeRegressor()\n",
    "        self.right.fit(X[right_indices], y[right_indices])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.feature_index is None:\n",
    "            return self.value\n",
    "        \n",
    "        predictions = np.zeros(len(X))\n",
    "        left_indices = X[:, self.feature_index] < self.threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        predictions[left_indices] = self.left.predict(X[left_indices])\n",
    "        predictions[right_indices] = self.right.predict(X[right_indices])\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        best_loss = np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_index in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_index]\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            for threshold in unique_values:\n",
    "                left_indices = feature_values < threshold\n",
    "                right_indices = ~left_indices\n",
    "                \n",
    "                left_loss = np.mean((y[left_indices] - np.mean(y[left_indices])) ** 2)\n",
    "                right_loss = np.mean((y[right_indices] - np.mean(y[right_indices])) ** 2)\n",
    "                total_loss = left_loss + right_loss\n",
    "                \n",
    "                if total_loss < best_loss:\n",
    "                    best_loss = total_loss\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature_index, best_threshold, None\n",
    "\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = np.mean((y - model.predict(X)) ** 2)\n",
    "ssr = np.sum((y - model.predict(X)) ** 2)\n",
    "sst = np.sum((y - np.mean(y)) ** 2)\n",
    "r_squared = 1 - ssr / sst\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59183bb-7e80-48ce-b158-f7ca1679ce4a",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c17229-461c-474b-b56e-2c1a41892d0e",
   "metadata": {},
   "source": [
    "Ans - In Gradient Boosting, a weak learner is a simple predictive model that performs only slightly better than random guessing. These models typically have high bias, meaning they make systematic errors in their predictions, but low variance, meaning their predictions are consistent across different datasets. Common examples of weak learners include shallow decision trees (with only a few levels) or linear regression models with limited features. While weak learners may not be accurate on their own, they serve as the building blocks for Gradient Boosting. By sequentially combining multiple weak learners and focusing on correcting their errors, Gradient Boosting creates a strong, accurate model that can capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1abbab-2e5a-4e79-9995-1d15c7c5408e",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733f673-5e7d-4dc8-8705-56eebc4e30ec",
   "metadata": {},
   "source": [
    "Ans - Gradient Boosting's intuition lies in its iterative approach to reducing errors. It starts with a simple model that makes a preliminary prediction.  Then, it analyzes the errors in this prediction and constructs a second model that focuses on correcting these errors. This second model is added to the first, creating an ensemble. The process repeats, with each new model targeting the residual errors of the ensemble. In essence, Gradient Boosting is like a sculptor refining a statue; each successive model chisels away at the errors of the previous one, ultimately leading to a more accurate final prediction. The use of gradients, or the direction and magnitude of the steepest ascent of a function, guides the learning process. This allows the algorithm to focus on areas where the model is performing poorly, gradually improving its overall accuracy. The final prediction is a weighted combination of the predictions of all models, with better-performing models given more weight. This weighted average helps to create a robust and accurate predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3fa51-f9a2-4d1b-96bf-dc6985c7ecf8",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bdcbb-fd95-4d9f-b38c-5f7c10893526",
   "metadata": {},
   "source": [
    "Ans - Sequential Ensemble Learning It is a boosting technique where the outputs from individual weak learners associate sequentially during the training phase. The performance of the model is boosted by assigning higher weights to the samples that are incorrectly classified.\n",
    "\n",
    "1] Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, typically a decision tree with a small depth, called the base learner.\n",
    "\n",
    "2] Fit the base learner: The base learner is fitted to the training data, and its predictions are computed.\n",
    "\n",
    "3] Compute the residual errors: The difference between the actual target values and the predictions of the base learner is calculated. These differences are referred to as residual errors.\n",
    "\n",
    "4] Fit the next weak learner: A new weak learner is fitted to the residual errors. The objective of this weak learner is to learn the patterns in the residual errors that were not captured by the previous base learner.\n",
    "\n",
    "5] Update the ensemble: The predictions of the new weak learner are added to the predictions of the previous base learner. This update is done by multiplying the predictions of the weak learner by a small learning rate, which controls the contribution of each weak learner to the ensemble.\n",
    "\n",
    "6] Repeat steps 3 to 5: The process is repeated by computing the residual errors based on the updated ensemble's predictions and fitting a new weak learner to the residual errors. This process continues for a specified number of iterations or until a stopping criterion is met.\n",
    "\n",
    "7] Final ensemble prediction: The final prediction of the gradient boosting ensemble is obtained by summing the predictions of all the weak learners, weighted by their learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8ef90-235b-4245-acb8-960c255079d4",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995f229-d6c7-48ee-97b7-3f7e9b764a04",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression begins by establishing a way to measure the accuracy of its predictions. This is done by choosing a loss function that quantifies the difference between the predicted and actual target values. Common choices for regression include mean squared error (MSE) and mean absolute error (MAE).\n",
    "\n",
    "Next, it starts with a simple model, often a shallow decision tree or a constant value. This initial model isn't expected to be very accurate, but it provides a baseline for improvement.\n",
    "\n",
    "The algorithm then calculates the negative gradient of the loss function concerning the current model's predictions. This negative gradient acts as a guide, pointing the algorithm toward the direction where the error is steepest. In other words, it helps the algorithm identify where the current model is making the biggest mistakes.\n",
    "\n",
    "A weak learner, typically a decision tree, is then trained on this negative gradient. The goal is to have this weak learner approximate the negative gradient, effectively correcting the errors of the previous model.\n",
    "\n",
    "The predictions of the weak learner are then scaled down by a learning rate, which controls the contribution of each weak learner to the overall model. These scaled predictions are added to the predictions of the current model, updating it to be more accurate.\n",
    "\n",
    "This process of calculating the negative gradient, training a weak learner, and updating the model is repeated iteratively. Each new weak learner focuses on correcting the errors of the previous ensemble.\n",
    "\n",
    "Finally, the predictions of all the weak learners, each weighted by their learning rate, are combined to form the final prediction of the Gradient Boosting ensemble. This ensemble model is significantly more accurate than any single weak learner, as it has learned from the mistakes of all the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32410eae-3120-4256-bd3b-4ff8ec4f09b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0735f-ba96-4d35-a2ba-b08ce91d265f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
